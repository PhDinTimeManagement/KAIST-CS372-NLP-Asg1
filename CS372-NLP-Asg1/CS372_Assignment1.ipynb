{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLUrb_dUtOti"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "You must submit two materials for HW1 on KLMS **by March 28 at 23:59:59**, with penalty for late submission only over two days \\(-20%, -40%\\).\n",
    "\n",
    "<br>\n",
    "\n",
    "### A. ```.ipynb``` file containing your Python code\n",
    "\n",
    "- Provide your code below each problem. After completing each problem, make sure to **run the code and print the results**. Your submitted code must be executable.\n",
    "- You are allowed to use more than one code block per problem.\n",
    "\n",
    "### B. Report\n",
    "\n",
    "- Submit a report that describes the process and results of solving each problem (either with screenshots or text).\n",
    "- There is no page limit, but the report must be in PDF format.\n",
    "- Avoid unnecessary explanations for simple problems, but make sure to include essential details on how you solved each problem.\n",
    "- The report must be written **in English only**. Other languages are not allowed.\n",
    "\n",
    "### C. File Naming Convention\n",
    "Your file names must follow the format:\n",
    "- ```CS372_HW1_{studentID}.ipynb```\n",
    "- ```CS372_HW1_{studentID}.pdf```\n",
    "\n",
    "<br>\n",
    "\n",
    "Any form of plagiarism will be penalized.\n",
    "\n",
    "If you have any questions, please use the KLMS Q&A board or contact cs372@nlp.kaist.ac.kr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hlkhnEhvktT"
   },
   "source": [
    "## Example of Using Colab\n",
    "\n",
    "To use Colab, write your code in a code box and click the play button on the left to execute it. Below is a simple code -- click the play button to see how it prints the results.\n",
    "\n",
    "You can also add code cells or text by clicking ```+code``` or ```+text``` between blocks.\n",
    "\n",
    "Feel free to modify the code below to get familiar with Colab.\n",
    "\n",
    "The execution order of code cells may affect subsequent code, so make sure that different problems do not interfere with each other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rySKo_EqwaBK",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:40.093688Z",
     "start_time": "2025-03-28T01:14:38.623116Z"
    }
   },
   "source": [
    "from numba.core.ir import Print\n",
    "from sympy import limit\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "a[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY8QMl-pwgt6"
   },
   "source": [
    "## Environment Setting\n",
    "\n",
    "The downloads and imports below are baseline settings for the assignment. Once you are connected to the server, you must download these files; otherwise, you may encounter errors such as \"Resource words not found.\"\n",
    "\n",
    "If you need additional libraries or packages for solving the problems, you are allowed to use them. However, you **must** mention why you used them in your report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zOHN5SI1sXOk",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:41.514065Z",
     "start_time": "2025-03-28T01:14:40.111430Z"
    }
   },
   "source": [
    "import nltk, re, pprint\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download('words')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "nltk.download('udhr')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3KCI6a6zNKL"
   },
   "source": [
    "For Problems 1, 2 and 3, you need to import ```nltk.book``` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U3Zky5UrzaLD",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:43.505407Z",
     "start_time": "2025-03-28T01:14:41.660912Z"
    }
   },
   "source": [
    "from nltk.book import *"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ysAQ9h2qzplD",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:43.519121Z",
     "start_time": "2025-03-28T01:14:43.516812Z"
    }
   },
   "source": [
    "sent1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iBlLCtJVztPj",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:44.801089Z",
     "start_time": "2025-03-28T01:14:44.798548Z"
    }
   },
   "source": [
    "text1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problems"
   ],
   "metadata": {
    "id": "tJwaKaxhPYYj"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I87Lrsrtz3YM"
   },
   "source": [
    "### Problem 1 (2 points)\n",
    "\n",
    "\n",
    "Write a Python code that prints out a sorted list of unique words \\(excluding special characters\\) from the sentences ```sent1```, ```sent2```, ..., ```sent9``` combined, using list addition and sorted() operations. Additionally, print the size of the sorted list of unique words.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "86SnMxZyz85l",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:44.876414Z",
     "start_time": "2025-03-28T01:14:44.873984Z"
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "# Step 1: Combine all sentences using list addition\n",
    "all_sents = sent1 + sent2 + sent3 + sent4 + sent5 + sent6 + sent7 + sent8 + sent9\n",
    "\n",
    "# Step 2: Remove special characters, keep alphabetic and numeric words\n",
    "# Here we assume COVID19 is a valid word\n",
    "all_words = [word.lower() for word in all_sents if word.isalnum()]\n",
    "\n",
    "# Step 3: Get the unique words and convert it to a list\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Step 4: Sort the unique words in alphabetical order\n",
    "sorted_unique_words = sorted(unique_words)\n",
    "\n",
    "# Step 5: Print the sorted list of unique words\n",
    "print(f\"Sorted list of unique words: {sorted_unique_words}\")\n",
    "\n",
    "# Step 6: Print the size of the sorted list\n",
    "print(f\"Size of the sorted list: {len(sorted_unique_words)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted list of unique words: ['1', '25', '29', '61', 'a', 'and', 'arthur', 'as', 'attrac', 'been', 'beginning', 'board', 'call', 'citizens', 'clop', 'cloud', 'created', 'dashwood', 'director', 'discreet', 'earth', 'encounters', 'family', 'fellow', 'for', 'god', 'had', 'have', 'heaven', 'house', 'i', 'in', 'ishmael', 'join', 'king', 'lady', 'lay', 'lol', 'london', 'long', 'male', 'me', 'nonexecutive', 'of', 'old', 'older', 'on', 'park', 'people', 'pierre', 'pming', 'problem', 'ragged', 'red', 'representatives', 'saffron', 'scene', 'seeks', 'senate', 'settled', 'sexy', 'side', 'single', 'suburb', 'sunset', 'sussex', 'the', 'there', 'to', 'vinken', 'whoa', 'will', 'wind', 'with', 'years']\n",
      "Size of the sorted list: 75\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3fxRCWSz6Ob"
   },
   "source": [
    "### Problem 2 (2 points)\n",
    "\n",
    "Write a Python code that extracts the last two words of ```text2``` using the slice expression.\n"
   ]
  },
  {
   "metadata": {
    "id": "L2WRhN3t0EEd",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:44.906498Z",
     "start_time": "2025-03-28T01:14:44.904734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Problem 2\n",
    "\n",
    "# Step1: Extract the last two words of text2 using the slice expression\n",
    "last2_words = text2[-2:]\n",
    "\n",
    "# Step 2: Print the last two words\n",
    "# Optional for debugging, since the problem does not require any output\n",
    "# print(last2_words)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReMNzsql0G6T"
   },
   "source": [
    "### Problem 3 (4 points)\n",
    "\n",
    "Define a Python function called ```vocab_size(text)``` that takes a single parameter for the text and returns the vocabulary \\(excluding special characters\\) size of the input text. As input, use a list of words that concatenates ```sent1```, ```sent2```, ..., ```sent9```.\n",
    "\n",
    "Report the result of the function for two cases:\n",
    "- The result when the function is applied to the input data in its original state.\n",
    "- The result when the function is applied to the input data after changing all words to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jXki8mjq0WEa",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:44.932946Z",
     "start_time": "2025-03-28T01:14:44.930008Z"
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "# Step 1: Concatenate all sents as input data\n",
    "all_sents = sent1 + sent2 + sent3 + sent4 + sent5 + sent6 + sent7 + sent8 + sent9\n",
    "\n",
    "# Step 2: Define a helper function to remove special characters in a word\n",
    "# Here we assume COVID-19 is a valid word with meaning, but not %^&*\n",
    "def remove_special_chars(token):\n",
    "    import re\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', token)\n",
    "\n",
    "# Step 3: Define vocab_size(text) function\n",
    "def vocab_size(text):\n",
    "    # Declare a set to store the unique words\n",
    "    vocab = set()\n",
    "    for word in text:\n",
    "        # Preprocess the word by removing special characters\n",
    "        cleaned = remove_special_chars(word)\n",
    "        \n",
    "        # Check if the token after processing is not empty\n",
    "        # and contains only alphabets and digits\n",
    "        if cleaned and cleaned.isalnum():\n",
    "            vocab.add(cleaned)\n",
    "    \n",
    "    # Return the size of the vocabulary\n",
    "    return len(vocab)\n",
    "\n",
    "# Step 4: Apply function to the input data in its original state\n",
    "vocab_size_original = vocab_size(all_sents)\n",
    "print(f\"Vocabulary size in original state: {vocab_size_original}\")\n",
    "\n",
    "# Step 5: Apply function to the input data after changing all words to lowercase\n",
    "all_sents_lower = [word.lower() for word in all_sents]\n",
    "vocab_size_lower = vocab_size(all_sents_lower)\n",
    "print(f\"Vocabulary size after changing all words to lowercase: {vocab_size_lower}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size in original state: 80\n",
      "Vocabulary size after changing all words to lowercase: 76\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZOsuVpz0aDk"
   },
   "source": [
    "### Problem 4 (4 points)\n",
    "\n",
    "Assume that the following list of words is given:\n",
    "\n",
    "`['chair', 'teacher', 'chapter', 'echo', 'cider', 'china', 'camera', 'character', 'beach', 'approach']`\n",
    "\n",
    "Write a Python code to perform the following tasks:\n",
    "\n",
    "a. Print all words that begins with ```\"ch\"```. (2 points)\n",
    "\n",
    "b. Print all words that are longer than four characters. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FaW534DP0jxz",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:46.102496Z",
     "start_time": "2025-03-28T01:14:46.099290Z"
    }
   },
   "source": [
    "# Problem 4\n",
    "\n",
    "word_list = ['chair', 'teacher', 'chapter', 'echo', 'cider', 'china', 'camera', 'character', 'beach', 'approach']\n",
    "\n",
    "# Step 1: Declare two lists to store the results for each task\n",
    "start_with_ch_words = []\n",
    "long_words = []\n",
    "\n",
    "for word in word_list:\n",
    "    # Step 2: Find words that start with 'ch'\n",
    "    if word.startswith('ch'):\n",
    "        start_with_ch_words.append(word)\n",
    "    # Step 3: Find the words that longer than 4 characters\n",
    "    if len(word) > 4:\n",
    "        long_words.append(word)\n",
    "\n",
    "# Step 4: Print the results for each task\n",
    "print(f\"Words that start with 'ch': {start_with_ch_words}\")\n",
    "print(f\"Words that are longer than 4 characters: {long_words}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that start with 'ch': ['chair', 'chapter', 'china', 'character']\n",
      "Words that are longer than 4 characters: ['chair', 'teacher', 'chapter', 'cider', 'china', 'camera', 'character', 'beach', 'approach']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOUi8BqB08CE"
   },
   "source": [
    "### Problem 5 (3 points)\n",
    "\n",
    "We want to identify special bigrams that occur in the same sentence, with exactly two words between them. For example, in the sentence \"Success comes to those who work.\", the following three special bigram pairs are considered special:\n",
    "- \\(```Success```, ```those```\\)\n",
    "- \\(```comes```, ```who```\\)\n",
    "- \\(```to```, ```work```\\)\n",
    "\n",
    "Find the 10 most frequent special bigrams in the News category of the Brown Corpus and show the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-Nqx3_tx07jE",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:14:47.518809Z",
     "start_time": "2025-03-28T01:14:47.392915Z"
    }
   },
   "source": [
    "# Problem 5\n",
    "\n",
    "# Step 1: Load Brown Corpus's News Category\n",
    "from nltk.corpus import brown\n",
    "news_sents = brown.sents(categories='news')\n",
    "\n",
    "# Step 2: Declare a dictionary to store the bigram frequency\n",
    "bigram_freq = {}\n",
    "\n",
    "# Step 3: Find special bigrams with only alphabet and numerical characters\n",
    "for sent in news_sents:\n",
    "    for i in range(len(sent) - 3):\n",
    "        # Convert the words to lowercase\n",
    "        w1 = sent[i].lower()\n",
    "        w2 = sent[i+1].lower()\n",
    "        w3 = sent[i+2].lower()\n",
    "        w4 = sent[i+3].lower()\n",
    "        \n",
    "        # Check if the words are all in alphanumeric format\n",
    "        if w1.isalnum() and w2.isalnum() and w3.isalnum() and w4.isalnum():\n",
    "            bigram = (w1, w4)\n",
    "            bigram_freq[bigram] = bigram_freq.get(bigram, 0) + 1\n",
    "            \n",
    "# Step 4: Sort the bigrams by frequency\n",
    "sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 5: Print top 10 most frequent special bigrams\n",
    "print(f\"Top 10 most frequent special bigrams in the News category of the Brown Corpus:\\n\", sorted_bigrams[:10])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent special bigrams in the News category of the Brown Corpus:\n",
      " [(('the', 'the'), 405), (('the', 'of'), 254), (('a', 'the'), 137), (('the', 'a'), 126), (('the', 'in'), 108), (('in', 'of'), 103), (('and', 'of'), 101), (('to', 'the'), 101), (('and', 'the'), 95), (('the', 'to'), 85)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2MHKTnN1Eld"
   },
   "source": [
    "### Problem 6 (3 points)\n",
    "\n",
    "Show how many words in the Gutenberg shakespeare-macbeth corpus have two or more pronunciations according to the CMU Pronouncing Dictionary. Additionally, show the list of words with the most diverse pronunciations.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fkj9fJYW1DzD",
    "ExecuteTime": {
     "end_time": "2025-03-28T01:20:22.354601Z",
     "start_time": "2025-03-28T01:20:22.038054Z"
    }
   },
   "source": [
    "# Problem 6\n",
    "\n",
    "# Step 1: Load Shakespeare Macbeth Corpus and preprocess the words\n",
    "from nltk.corpus import gutenberg\n",
    "# Use set to store unique words\n",
    "macbeth_words = set(\n",
    "    # Convert to lowercase and remove non-alphabetic words\n",
    "    w.lower()\n",
    "    for w in gutenberg.words('shakespeare-macbeth.txt')\n",
    "    if w.isalpha()\n",
    ")\n",
    "\n",
    "# Step 2: Load CMU Pronouncing Dictionary\n",
    "from nltk.corpus import cmudict\n",
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "# Step 3: Find words with >= 2 pronunciations\n",
    "multi_pron_words = []\n",
    "for word in macbeth_words:\n",
    "    if word in cmu_dict and len(cmu_dict[word]) >= 2:\n",
    "        multi_pron_words.append(word)\n",
    "        \n",
    "# Step 4: Print the total number of words with >= 2 pronunciations\n",
    "print(f\"Number of words with two or more pronunciations: {len(multi_pron_words)}\")\n",
    "\n",
    "# Step 5: Find word with the most diverse pronunciations\n",
    "max_pronunciation_word = []\n",
    "max_pronunciation_count = 0\n",
    "for word in multi_pron_words:\n",
    "    # Check if the word has more pronunciations than the current max\n",
    "    if len(cmu_dict[word]) > max_pronunciation_count:\n",
    "        # Update the max count\n",
    "        max_pronunciation_count = len(cmu_dict[word])\n",
    "        # Reset the list with the new word\n",
    "        max_pronunciation_word = [word]\n",
    "    # If the word has the same number of pronunciations as the current max\n",
    "    elif len(cmu_dict[word]) == max_pronunciation_count:\n",
    "        # Append the word to the list\n",
    "        max_pronunciation_word.append(word)\n",
    "\n",
    "# Step 6: Print the list of words with the most diverse pronunciations\n",
    "print(f\"Words with the most diverse pronunciations:\\n {max_pronunciation_word} ({max_pronunciation_count} pronunciations)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "Number of words with two or more pronunciations: 304\n",
      "Words with the most diverse pronunciations:\n",
      " ['direction', 'directly', 'was', 'effects', 'painted', 'interest', 'ne', 'with', 'when', 'planted'] (4 pronunciations)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecuw59je1I6D"
   },
   "source": [
    "### Problem 7 (4 points)\n",
    "\n",
    "\n",
    "Provide a brief explanation for each of the following questions.\n",
    "\n",
    "a. Retrieve all noun synsets for ```duck``` from WordNet. For each synset, print all hypernyms and the lemma names of those hypernyms. (2 points)\n",
    "\n",
    "b. Using ```wn.all_synsets('v')```, write a Python code that prints 5 random lemmas from the first 10 verb synsets. (The output lemma will be in the format: ```Lemma('run.v.02.run')```) (2 points)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Problem 7-a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:25.937462Z",
     "start_time": "2025-03-26T12:10:25.897062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Problem 7\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Step 1: Load WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Step 2: Retrieve all noun synsets for duck\n",
    "duck_synsets = wn.synsets('duck', pos='n')\n",
    "\n",
    "# Step 3: Print hypernyms and lemma names of hypernyms\n",
    "for synset in duck_synsets:\n",
    "    print(f\"Synset: {synset.name()} - {synset.definition()}\")\n",
    "    hypernyms = synset.hypernyms()\n",
    "    # Deffensive check if there are no hypernyms\n",
    "    if not hypernyms:\n",
    "        print(\"No hypernymsfor this synset.\\n\")\n",
    "        continue\n",
    "        \n",
    "    for hypernym in hypernyms:\n",
    "        print(f\"Hypernym: {hypernym.name()} Lemma names - {hypernym.lemma_names()}\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: duck.n.01 - small wild or domesticated web-footed broad-billed swimming bird usually having a depressed body and short legs\n",
      "Hypernym: anseriform_bird.n.01 Lemma names - ['anseriform_bird']\n",
      "\n",
      "Synset: duck.n.02 - (cricket) a score of nothing by a batsman\n",
      "Hypernym: score.n.03 Lemma names - ['score']\n",
      "\n",
      "Synset: duck.n.03 - flesh of a duck (domestic or wild)\n",
      "Hypernym: poultry.n.02 Lemma names - ['poultry']\n",
      "\n",
      "Synset: duck.n.04 - a heavy cotton fabric of plain weave; used for clothing and tents\n",
      "Hypernym: fabric.n.01 Lemma names - ['fabric', 'cloth', 'material', 'textile']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Problem 7-b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:26.274180Z",
     "start_time": "2025-03-26T12:10:25.968760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Retrieve all verb synsets\n",
    "verb_synsets = list(wn.all_synsets('v'))\n",
    "\n",
    "# Step 2: Make counter to print only first 10 synsets\n",
    "counter = 0\n",
    "limit = 10\n",
    "\n",
    "# Step 2 : Iterate over 10 synsets and print 5 random lemmas\n",
    "for idx, synset in enumerate(verb_synsets, start=1):\n",
    "    all_lemmas = synset.lemmas()\n",
    "    \n",
    "    if len(all_lemmas) < 5:\n",
    "        # print(f\"Synset {idx} has less than 5 lemmas.\")\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        random_lemmas = random.sample(all_lemmas, 5)\n",
    "    \n",
    "    print(f\"Synset #{idx}: {synset.name()} -> definition: {synset.definition()}\")\n",
    "    for lemma in random_lemmas:\n",
    "        print(f\"  {lemma}\")\n",
    "    print()\n",
    "    \n",
    "    counter += 1\n",
    "    if counter == limit:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset #20: expectorate.v.02 -> definition: discharge (phlegm or sputum) from the lungs and out of the mouth\n",
      "  Lemma('expectorate.v.02.expectorate')\n",
      "  Lemma('expectorate.v.02.spit_out')\n",
      "  Lemma('expectorate.v.02.cough_out')\n",
      "  Lemma('expectorate.v.02.cough_up')\n",
      "  Lemma('expectorate.v.02.spit_up')\n",
      "\n",
      "Synset #36: shed.v.04 -> definition: cast off hair, skin, horn, or feathers\n",
      "  Lemma('shed.v.04.molt')\n",
      "  Lemma('shed.v.04.slough')\n",
      "  Lemma('shed.v.04.exuviate')\n",
      "  Lemma('shed.v.04.shed')\n",
      "  Lemma('shed.v.04.moult')\n",
      "\n",
      "Synset #63: sleep.v.01 -> definition: be asleep\n",
      "  Lemma('sleep.v.01.catch_some_Z's')\n",
      "  Lemma('sleep.v.01.log_Z's')\n",
      "  Lemma('sleep.v.01.sleep')\n",
      "  Lemma('sleep.v.01.slumber')\n",
      "  Lemma('sleep.v.01.kip')\n",
      "\n",
      "Synset #76: fall_asleep.v.01 -> definition: change from a waking to a sleeping state\n",
      "  Lemma('fall_asleep.v.01.drift_off')\n",
      "  Lemma('fall_asleep.v.01.dope_off')\n",
      "  Lemma('fall_asleep.v.01.nod_off')\n",
      "  Lemma('fall_asleep.v.01.drop_off')\n",
      "  Lemma('fall_asleep.v.01.fall_asleep')\n",
      "\n",
      "Synset #79: go_to_bed.v.01 -> definition: prepare for sleep\n",
      "  Lemma('go_to_bed.v.01.go_to_sleep')\n",
      "  Lemma('go_to_bed.v.01.crawl_in')\n",
      "  Lemma('go_to_bed.v.01.hit_the_sack')\n",
      "  Lemma('go_to_bed.v.01.turn_in')\n",
      "  Lemma('go_to_bed.v.01.retire')\n",
      "\n",
      "Synset #80: get_up.v.02 -> definition: get up and out of bed\n",
      "  Lemma('get_up.v.02.rise')\n",
      "  Lemma('get_up.v.02.arise')\n",
      "  Lemma('get_up.v.02.get_up')\n",
      "  Lemma('get_up.v.02.uprise')\n",
      "  Lemma('get_up.v.02.turn_out')\n",
      "\n",
      "Synset #82: wake_up.v.02 -> definition: stop sleeping\n",
      "  Lemma('wake_up.v.02.arouse')\n",
      "  Lemma('wake_up.v.02.waken')\n",
      "  Lemma('wake_up.v.02.awake')\n",
      "  Lemma('wake_up.v.02.wake')\n",
      "  Lemma('wake_up.v.02.awaken')\n",
      "\n",
      "Synset #83: awaken.v.01 -> definition: cause to become awake or conscious\n",
      "  Lemma('awaken.v.01.awaken')\n",
      "  Lemma('awaken.v.01.arouse')\n",
      "  Lemma('awaken.v.01.rouse')\n",
      "  Lemma('awaken.v.01.wake_up')\n",
      "  Lemma('awaken.v.01.wake')\n",
      "\n",
      "Synset #94: anesthetize.v.01 -> definition: administer an anesthetic drug to\n",
      "  Lemma('anesthetize.v.01.anesthetise')\n",
      "  Lemma('anesthetize.v.01.put_under')\n",
      "  Lemma('anesthetize.v.01.put_out')\n",
      "  Lemma('anesthetize.v.01.anesthetize')\n",
      "  Lemma('anesthetize.v.01.anaesthetize')\n",
      "\n",
      "Synset #100: sedate.v.01 -> definition: cause to be calm or quiet as by administering a sedative to\n",
      "  Lemma('sedate.v.01.tranquillise')\n",
      "  Lemma('sedate.v.01.sedate')\n",
      "  Lemma('sedate.v.01.calm')\n",
      "  Lemma('sedate.v.01.tranquillize')\n",
      "  Lemma('sedate.v.01.tranquilize')\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FvQLJEM1aI8"
   },
   "source": [
    "### Problem 8 (3 points)\n",
    "\n",
    "Describe the class of strings that match each of the following regular expressions.\n",
    "1.\t[a-zA]+\n",
    "2.\t[a-z][A-Z]*[a-z]\n",
    "3.\t[BCLT]r[aeiou]{,2}t[a-z]\n",
    "4.\t([aeiou][^aeiou])*\n",
    "5.\t\\W+|[^\\W\\s]+\n",
    "\n",
    "You can test your answers using ```nltk.re_show()```.\n",
    "Your answers should be included in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rrkrUCOV1iQ8",
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:26.309619Z",
     "start_time": "2025-03-26T12:10:26.307192Z"
    }
   },
   "source": [
    "# Problem 8\n",
    "\n",
    "# Step 1: Import nltk\n",
    "import nltk\n",
    "\n",
    "# Step 2: Define the regular expressions\n",
    "regexps = [\n",
    "    r\"[a-zA]+\",\n",
    "    r\"[a-z][A-Z]*[a-z]\",\n",
    "    r\"[BCLT]r[aeiou]{,2}t[a-z]\",\n",
    "    r\"([aeiou][^aeiou])*\",\n",
    "    r\"\\W+|[^\\W\\s]+\"\n",
    "]\n",
    "\n",
    "# Step 3: Test the regular expressions\n",
    "for regexp in regexps:\n",
    "    print(f\"Regular Expression: {regexp}\")\n",
    "    nltk.re_show(regexp, \"Your answers should be included in the report.\")\n",
    "    print()\n",
    "    \n",
    "# Step 4: Describe the class of strings\n",
    "# 1. [a-zA]+: One or more alphabetic characters\n",
    "# 2. [a-z][A-Z]*[a-z]: One lowercase letter, followed by zero or more uppercase letters, and then another lowercase letter\n",
    "# 3. [BCLT]r[aeiou]{,2}t[a-z]: A single uppercase letter from B, C, L, or T, followed by the letter 'r', followed by up to 2 vowels, followed by the letter 't', and ending with any lowercase letter\n",
    "# 4. ([aeiou][^aeiou])*: Zero or more pairs of a vowel followed by a non-vowel\n",
    "# 5. \\W+|[^\\W\\s]+: One or more non-alphanumeric characters or one or more non-whitespace characters\n",
    "#"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression: [a-zA]+\n",
      "Y{our} {answers} {should} {be} {included} {in} {the} {report}.\n",
      "\n",
      "Regular Expression: [a-z][A-Z]*[a-z]\n",
      "Y{ou}r {an}{sw}{er}s {sh}{ou}{ld} {be} {in}{cl}{ud}{ed} {in} {th}e {re}{po}{rt}.\n",
      "\n",
      "Regular Expression: [BCLT]r[aeiou]{,2}t[a-z]\n",
      "Your answers should be included in the report.\n",
      "\n",
      "Regular Expression: ([aeiou][^aeiou])*\n",
      "{}Y{}o{ur}{} {an}{}s{}w{er}{}s{} {}s{}h{}o{ul}{}d{} {}b{e in}{}c{}l{uded}{} {in}{} {}t{}h{e }{}r{epor}{}t{}.{}\n",
      "\n",
      "Regular Expression: \\W+|[^\\W\\s]+\n",
      "{Your}{ }{answers}{ }{should}{ }{be}{ }{included}{ }{in}{ }{the}{ }{report}{.}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNet6GaG10Gk"
   },
   "source": [
    "### Problem 9 (5 points)\n",
    "\n",
    "Pig Latin is a simple transformation of English text. Each word is converted as follows:\n",
    "- Move all consonant (or consonant cluster) at the start of a word to the end.\n",
    "- Append ```ay``` to the word.\n",
    "- Example:\n",
    "  - ```string``` → ```ingstray```\n",
    "  - ```language``` → ```anguagelay```\n",
    "  - ```idle``` → ```idleay```\n",
    "  - For more details, refer to http://en.wikipedia.org/wiki/Pig_Latin.\n",
    "\n",
    "This time, we will modify the Pig Latin rules:\n",
    "- After moving the consonant(s) of the word, change the first vowel in the word:\n",
    "    - ```a``` → ```e```\n",
    "    - ```e``` → ```i```\n",
    "    - ```i``` → ```o```\n",
    "    - ```o``` → ```u```\n",
    "    - ```u``` → ```a```\n",
    "- Append ```ay``` to the word.\n",
    "- Example:\n",
    "    - ```string``` → ```ongstray```\n",
    "    - ```idle``` → ```odleay```\n",
    "\n",
    "a. Write a Python function to convert a word into its equivalent in the modified Pig Latin. (3 points)\n",
    "\n",
    "b. Write a Python code that converts an entire text (a sequence of words), instead of just individual words.  (1 point)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Problem 9-a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "irIbiO6U19Mc",
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:26.343512Z",
     "start_time": "2025-03-26T12:10:26.340912Z"
    }
   },
   "source": [
    "# Problem 9\n",
    "\n",
    "# Step 1: Define function to convert a word into Pig Latin\n",
    "def pig_latin(word):\n",
    "    vowels = 'aeiou'\n",
    "    consonants = ''\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            # Mark down the char's idx in vowels\n",
    "            pos = vowels.index(char)\n",
    "            if pos == len(vowels) - 1:\n",
    "                pos = -1\n",
    "            # print(pos) # Debugging line\n",
    "            break\n",
    "        consonants += char\n",
    "        \n",
    "    idx = (pos + 1) % len(vowels)\n",
    "    # print(idx) # Debugging line\n",
    "    \n",
    "    if word[0] in vowels:\n",
    "        return vowels[idx] + word[1:] + 'ay'\n",
    "    else:\n",
    "        return vowels[idx] + word[len(consonants)+1:] + consonants + 'ay'\n",
    "\n",
    "# Testing    \n",
    "print(pig_latin('string')) # ongstray\n",
    "print(pig_latin('language')) # enguagelay\n",
    "print(pig_latin('idle')) # odleay"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ongstray\n",
      "enguagelay\n",
      "odleay\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Problem 9-b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:26.377786Z",
     "start_time": "2025-03-26T12:10:26.375834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Write a Python code that converts an entire text (a sequence of words), instead of just individual words.\"\n",
    "\n",
    "for word in text.split():\n",
    "    print(pig_latin(word), end=' ')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oteWray eay unPythay udecay etthay unvertscay enay intireay ixttay e(ay iquencesay ufay urds),way onsteaday ufay astjay ondividualay urds.way "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9tST3r52Dhk"
   },
   "source": [
    "### Problem 10 (4 points)\n",
    "\n",
    "Using a multilingual corpus such as the Universal Declaration of Human Rights Corpus (```nltk.corpus.udhr```), along with NLTK’s frequency distribution (```nltk.FreqDist```), develop a system that guesses the language of a previously unseen text.\n",
    "\n",
    "You can estimate the language of the unseen text by comparing:\n",
    "- How often each word appears in each language of the corpus\n",
    "- How often each word appears in the unseen text.\n",
    "\n",
    "To achieve this, use rank correlation (```nltk.spearman_correlation```).\n",
    "\n",
    "For simplicity, you can work at the character level rather than the word level and choose languages (at least two) of your choice.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:26.426067Z",
     "start_time": "2025-03-26T12:10:26.409014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Problem 10\n",
    "from nltk.metrics.spearman import spearman_correlation\n",
    "from nltk.corpus import udhr\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "languages = [\n",
    "    \"English-Latin1\",\n",
    "    \"French_Francais-Latin1\",\n",
    "    \"German_Deutsch-Latin1\",\n",
    "    \"Italian_Italiano-Latin1\",\n",
    "    \"Korean_Hankuko-UTF8\",\n",
    "    \"Japanese_Nihongo-UTF8\",\n",
    "    \"Spanish-Latin1\",\n",
    "    \"Swedish_Svenska-Latin1\"\n",
    "]\n",
    "\n",
    "unseen_text = \"You never really understand a person until you consider things from his point of view… Until you climb inside of his skin and walk around in it.\"\n",
    "\n",
    "\n",
    "def get_char_ranks(text, n=20):\n",
    "    # Create a frequency distribution of alphabetic characters\n",
    "    freqdist = FreqDist(ch for ch in text if ch.isalpha())\n",
    "    # Sort chars by descending frequency, take top n\n",
    "    most_common_chars = freqdist.most_common(n)\n",
    "    # Return just the chars in order\n",
    "    return [char for (char, _) in most_common_chars]\n",
    "\n",
    "def compute_spearman_rank_correlation(ranks1, ranks2):\n",
    "    # Create dictionaries: char -> rank_index\n",
    "    dict1 = {ch: i for i, ch in enumerate(ranks1)}\n",
    "    dict2 = {ch: i for i, ch in enumerate(ranks2)}\n",
    "    \n",
    "    # We only want to compare characters common to both sets\n",
    "    common_chars = set(dict1.keys()) & set(dict2.keys())\n",
    "    \n",
    "    # Build sequences of (char, rank) pairs for the intersection\n",
    "    seq1 = [(ch, dict1[ch]) for ch in common_chars]\n",
    "    seq2 = [(ch, dict2[ch]) for ch in common_chars]\n",
    "    \n",
    "    if len(common_chars) < 2:\n",
    "        # Not enough overlap to compute correlation\n",
    "        return 0.0\n",
    "    \n",
    "    # Now pass these sequences to spearman_correlation\n",
    "    return spearman_correlation(seq1, seq2)\n",
    "\n",
    "# 1) Compute ranks for the unseen text\n",
    "unseen_ranks = get_char_ranks(unseen_text, n=20)\n",
    "print(\"Unseen text top-20 character ranks:\")\n",
    "print(unseen_ranks)\n",
    "print()\n",
    "\n",
    "# 2) For each language, compute top-n char ranks & measure correlation\n",
    "results = []\n",
    "for lang in languages:\n",
    "    lang_text = udhr.raw(lang)\n",
    "    lang_ranks = get_char_ranks(lang_text, n=20)\n",
    "    \n",
    "    corr_value = compute_spearman_rank_correlation(unseen_ranks, lang_ranks)\n",
    "    results.append((lang, corr_value))\n",
    "\n",
    "# 3) Print results\n",
    "print(\"Language Rank Correlations:\")\n",
    "for lang, corr in results:\n",
    "    print(f\"{lang}: Spearman correlation = {corr:.4f}\")\n",
    "\n",
    "# Identify the language with the highest correlation\n",
    "best_guess, best_corr = max(results, key=lambda x: x[1])\n",
    "print(f\"\\nBest guess for unseen text language: {best_guess} (corr = {best_corr:.4f})\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen text top-20 character ranks:\n",
      "['n', 'i', 'o', 'e', 's', 'r', 'u', 'a', 'l', 'd', 't', 'y', 'h', 'f', 'v', 'p', 'c', 'm', 'w', 'k']\n",
      "\n",
      "Language Rank Correlations:\n",
      "English-Latin1: Spearman correlation = 0.6966\n",
      "French_Francais-Latin1: Spearman correlation = 0.7221\n",
      "German_Deutsch-Latin1: Spearman correlation = 0.6581\n",
      "Italian_Italiano-Latin1: Spearman correlation = 0.7169\n",
      "Korean_Hankuko-UTF8: Spearman correlation = 0.0000\n",
      "Japanese_Nihongo-UTF8: Spearman correlation = 0.0000\n",
      "Spanish-Latin1: Spearman correlation = 0.7108\n",
      "Swedish_Svenska-Latin1: Spearman correlation = 0.3787\n",
      "\n",
      "Best guess for unseen text language: French_Francais-Latin1 (corr = 0.7221)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNwK-R-Z2Nnt"
   },
   "source": [
    "### Problem 11 (6 points)\n",
    "\n",
    "Write a Python program that processes a text and discovers cases where a word is used with a novel sense.\n",
    "\n",
    "One simple approach is using WordNet:\n",
    "For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context.\n",
    "\n",
    "Note: this is a crude approach. Also, it is important to mention that implementing this effectively is a difficult, open research problem.\n",
    "\n",
    "Define what a “novel sense” of the word is in your own words, and describe the detailed design choice you made for your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZY2Xrs2j2UON",
    "ExecuteTime": {
     "end_time": "2025-03-26T12:10:26.637137Z",
     "start_time": "2025-03-26T12:10:26.459700Z"
    }
   },
   "source": [
    "# Problem 11\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def avg_similarity(target_synset, context_synsets):\n",
    "    \"\"\"\n",
    "    Computes the average WordNet similarity between the target_synset\n",
    "    and all synsets in context_synsets. Returns 0.0 if no valid similarities.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for c_syn in context_synsets:\n",
    "        sim = target_synset.path_similarity(c_syn)\n",
    "        # You could also try wup_similarity or another measure\n",
    "        if sim is not None:\n",
    "            similarities.append(sim)\n",
    "    if similarities:\n",
    "        return sum(similarities) / len(similarities)\n",
    "    return 0.0\n",
    "\n",
    "def best_synset_for_context(word, context_words):\n",
    "    \"\"\"\n",
    "    Among all synsets for `word`, returns the one with highest average similarity\n",
    "    to the synsets of `context_words`.\n",
    "    \"\"\"\n",
    "    # Collect all synsets for each context word\n",
    "    context_syns = [syn for cw in context_words for syn in wn.synsets(cw)]\n",
    "    target_syns = wn.synsets(word)\n",
    "    \n",
    "    if not target_syns or not context_syns:\n",
    "        return None  # No synsets or no context synsets\n",
    "    \n",
    "    best_synset = None\n",
    "    best_score = 0.0\n",
    "    \n",
    "    for ts in target_syns:\n",
    "        score = avg_similarity(ts, context_syns)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_synset = ts\n",
    "    return best_synset\n",
    "\n",
    "def detect_novel_sense(text, window_size=2):\n",
    "    \"\"\"\n",
    "    A toy function that processes each word in the text,\n",
    "    checks if the used sense might be 'novel' by measuring\n",
    "    the best sense vs the worst sense.\n",
    "    \n",
    "    text: list of words (tokenized).\n",
    "    \"\"\"\n",
    "    novel_usages = []\n",
    "    \n",
    "    for i, word in enumerate(text):\n",
    "        # We'll assume that punctuation etc. is removed or skip them if no synsets\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        \n",
    "        # Grab the local context\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(text), i + window_size + 1)\n",
    "        context_words = [w for j, w in enumerate(text[start:end]) if j != i and w.isalpha()]\n",
    "        \n",
    "        # Calculate best sense for that context\n",
    "        best_sense = best_synset_for_context(word, context_words)\n",
    "        \n",
    "        if best_sense is None:\n",
    "            continue  # Could not find or evaluate senses for the context\n",
    "        \n",
    "        # Let's define 'novel' if the best sense is actually not the first sense in WordNet,\n",
    "        # or if there's some other naive test. We'll do something simplistic:\n",
    "        # Compare best_sense to the \"WordNet's default sense\" = word's first synset\n",
    "        # If they're different, let's mark it as 'novel' (Crude!)\n",
    "        \n",
    "        first_syn = wn.synsets(word)[0]  # WordNet's default sense\n",
    "        if best_sense != first_syn:\n",
    "            # We'll store (word, best_sense, first_syn)\n",
    "            novel_usages.append((word, best_sense, first_syn))\n",
    "    \n",
    "    return novel_usages\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    text = nltk.word_tokenize(\n",
    "        \"The bank of the river was very steep. I decided to deposit money at the bank. \"\n",
    "        \"Such a bank shot in billiards is too risky.\"\n",
    "    )\n",
    "    \n",
    "    novel_cases = detect_novel_sense(text, window_size=2)\n",
    "    print(\"Possible 'novel sense' usage (toy approach):\")\n",
    "    for (word, best_sense, first_sense) in novel_cases:\n",
    "        print(f\"Word '{word}' best-sense: {best_sense.name()} vs. WordNet-first: {first_sense.name()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible 'novel sense' usage (toy approach):\n",
      "Word 'was' best-sense: be.v.01 vs. WordNet-first: washington.n.02\n",
      "Word 'steep' best-sense: steep.a.01 vs. WordNet-first: steep.n.01\n",
      "Word 'I' best-sense: one.s.01 vs. WordNet-first: iodine.n.01\n",
      "Word 'deposit' best-sense: sediment.n.01 vs. WordNet-first: deposit.n.01\n",
      "Word 'bank' best-sense: bank.v.05 vs. WordNet-first: bank.n.01\n",
      "Word 'bank' best-sense: bank.v.05 vs. WordNet-first: bank.n.01\n",
      "Word 'shot' best-sense: changeable.s.04 vs. WordNet-first: shooting.n.01\n",
      "Word 'in' best-sense: in.s.01 vs. WordNet-first: inch.n.01\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
