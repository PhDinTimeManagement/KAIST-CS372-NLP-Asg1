{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLUrb_dUtOti"
   },
   "source": [
    "## Submission Instructions\n",
    "\n",
    "You must submit two materials for HW1 on KLMS **by March 28 at 23:59:59**, with penalty for late submission only over two days \\(-20%, -40%\\).\n",
    "\n",
    "<br>\n",
    "\n",
    "### A. ```.ipynb``` file containing your Python code\n",
    "\n",
    "- Provide your code below each problem. After completing each problem, make sure to **run the code and print the results**. Your submitted code must be executable.\n",
    "- You are allowed to use more than one code block per problem.\n",
    "\n",
    "### B. Report\n",
    "\n",
    "- Submit a report that describes the process and results of solving each problem (either with screenshots or text).\n",
    "- There is no page limit, but the report must be in PDF format.\n",
    "- Avoid unnecessary explanations for simple problems, but make sure to include essential details on how you solved each problem.\n",
    "- The report must be written **in English only**. Other languages are not allowed.\n",
    "\n",
    "### C. File Naming Convention\n",
    "Your file names must follow the format:\n",
    "- ```CS372_HW1_{studentID}.ipynb```\n",
    "- ```CS372_HW1_{studentID}.pdf```\n",
    "\n",
    "<br>\n",
    "\n",
    "Any form of plagiarism will be penalized.\n",
    "\n",
    "If you have any questions, please use the KLMS Q&A board or contact cs372@nlp.kaist.ac.kr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hlkhnEhvktT"
   },
   "source": [
    "## Example of Using Colab\n",
    "\n",
    "To use Colab, write your code in a code box and click the play button on the left to execute it. Below is a simple code -- click the play button to see how it prints the results.\n",
    "\n",
    "You can also add code cells or text by clicking ```+code``` or ```+text``` between blocks.\n",
    "\n",
    "Feel free to modify the code below to get familiar with Colab.\n",
    "\n",
    "The execution order of code cells may affect subsequent code, so make sure that different problems do not interfere with each other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rySKo_EqwaBK",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:33:50.784305Z",
     "start_time": "2025-03-25T08:33:50.779355Z"
    }
   },
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "a[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY8QMl-pwgt6"
   },
   "source": [
    "## Environment Setting\n",
    "\n",
    "The downloads and imports below are baseline settings for the assignment. Once you are connected to the server, you must download these files; otherwise, you may encounter errors such as \"Resource words not found.\"\n",
    "\n",
    "If you need additional libraries or packages for solving the problems, you are allowed to use them. However, you **must** mention why you used them in your report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zOHN5SI1sXOk",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:33:54.207363Z",
     "start_time": "2025-03-25T08:33:52.684352Z"
    }
   },
   "source": [
    "import nltk, re, pprint\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download('words')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "nltk.download('inaugural')\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('webtext')\n",
    "nltk.download('treebank')\n",
    "nltk.download('udhr')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     /Users/chloeyamtai/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3KCI6a6zNKL"
   },
   "source": [
    "For Problems 1, 2 and 3, you need to import ```nltk.book``` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U3Zky5UrzaLD",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:33:56.161683Z",
     "start_time": "2025-03-25T08:33:54.346543Z"
    }
   },
   "source": [
    "from nltk.book import *"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ysAQ9h2qzplD",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:33:57.388096Z",
     "start_time": "2025-03-25T08:33:57.385734Z"
    }
   },
   "source": [
    "sent1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iBlLCtJVztPj",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:33:57.400597Z",
     "start_time": "2025-03-25T08:33:57.398530Z"
    }
   },
   "source": [
    "text1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problems"
   ],
   "metadata": {
    "id": "tJwaKaxhPYYj"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I87Lrsrtz3YM"
   },
   "source": [
    "### Problem 1 (2 points)\n",
    "\n",
    "\n",
    "Write a Python code that prints out a sorted list of unique words \\(excluding special characters\\) from the sentences ```sent1```, ```sent2```, ..., ```sent9``` combined, using list addition and sorted() operations. Additionally, print the size of the sorted list of unique words.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "86SnMxZyz85l",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:33:57.419969Z",
     "start_time": "2025-03-25T08:33:57.417561Z"
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "# Step 1: Combine all sentences\n",
    "all_sents = sent1 + sent2 + sent3 + sent4 + sent5 + sent6 + sent7 + sent8 + sent9\n",
    "\n",
    "# Step 2: Remove special characters\n",
    "all_words = [word.lower() for word in all_sents if word.isalnum()]\n",
    "\n",
    "# Step 3: Get unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Step 4: Sort unique words\n",
    "sorted_unique_words = sorted(unique_words)\n",
    "\n",
    "# Step 5: Print result\n",
    "print(sorted_unique_words)\n",
    "\n",
    "# Step 6: Print the size of the sorted list\n",
    "print(len(sorted_unique_words))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '25', '29', '61', 'a', 'and', 'arthur', 'as', 'attrac', 'been', 'beginning', 'board', 'call', 'citizens', 'clop', 'cloud', 'created', 'dashwood', 'director', 'discreet', 'earth', 'encounters', 'family', 'fellow', 'for', 'god', 'had', 'have', 'heaven', 'house', 'i', 'in', 'ishmael', 'join', 'king', 'lady', 'lay', 'lol', 'london', 'long', 'male', 'me', 'nonexecutive', 'of', 'old', 'older', 'on', 'park', 'people', 'pierre', 'pming', 'problem', 'ragged', 'red', 'representatives', 'saffron', 'scene', 'seeks', 'senate', 'settled', 'sexy', 'side', 'single', 'suburb', 'sunset', 'sussex', 'the', 'there', 'to', 'vinken', 'whoa', 'will', 'wind', 'with', 'years']\n",
      "75\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3fxRCWSz6Ob"
   },
   "source": [
    "### Problem 2 (2 points)\n",
    "\n",
    "Write a Python code that extracts the last two words of ```text2``` using the slice expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L2WRhN3t0EEd",
    "ExecuteTime": {
     "end_time": "2025-03-25T08:37:30.510834Z",
     "start_time": "2025-03-25T08:37:30.507947Z"
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "# Step 1: Check text2 content\n",
    "print(text2)\n",
    "\n",
    "# Step2: Extract the last two words -2 and -1\n",
    "last2_words = text2[-2:]\n",
    "print(last2_words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Sense and Sensibility by Jane Austen 1811>\n",
      "['THE', 'END']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReMNzsql0G6T"
   },
   "source": [
    "### Problem 3 (4 points)\n",
    "\n",
    "Define a Python function called ```vocab_size(text)``` that takes a single parameter for the text and returns the vocabulary \\(excluding special characters\\) size of the input text. As input, use a list of words that concatenates ```sent1```, ```sent2```, ..., ```sent9```.\n",
    "\n",
    "Report the result of the function for two cases:\n",
    "- The result when the function is applied to the input data in its original state.\n",
    "- The result when the function is applied to the input data after changing all words to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jXki8mjq0WEa",
    "ExecuteTime": {
     "end_time": "2025-03-25T09:29:49.066809Z",
     "start_time": "2025-03-25T09:29:49.063681Z"
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "# Step 1: Concatenate input data\n",
    "all_sents = sent1 + sent2 + sent3 + sent4 + sent5 + sent6 + sent7 + sent8 + sent9\n",
    "\n",
    "# Step 2: Define vocab_size(text) function\n",
    "def vocab_size(text):\n",
    "    # Only keep alphabetic words\n",
    "    all_words = [word.lower() for word in text if word.isalpha()]\n",
    "\n",
    "    # Return the size of the unique words\n",
    "    return len(set(all_words))\n",
    "\n",
    "# Step 3: Apply vocab_size function\n",
    "print(vocab_size(all_sents))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZOsuVpz0aDk"
   },
   "source": [
    "### Problem 4 (4 points)\n",
    "\n",
    "Assume that the following list of words is given:\n",
    "\n",
    "`['chair', 'teacher', 'chapter', 'echo', 'cider', 'china', 'camera', 'character', 'beach', 'approach']`\n",
    "\n",
    "Write a Python code to perform the following tasks:\n",
    "\n",
    "a. Print all words that begins with ```\"ch\"```. (2 points)\n",
    "\n",
    "b. Print all words that are longer than four characters. (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FaW534DP0jxz",
    "ExecuteTime": {
     "end_time": "2025-03-25T11:40:49.759111Z",
     "start_time": "2025-03-25T11:40:49.756045Z"
    }
   },
   "source": [
    "# Problem 4\n",
    "word_list = ['chair', 'teacher', 'chapter', 'echo', 'cider', 'china', 'camera', 'character', 'beach', 'approach']\n",
    "\n",
    "# Step 1: Declare to lists to store the results\n",
    "start_with_ch_words = []\n",
    "long_words = []\n",
    "\n",
    "# Step 2: Find words that start with 'ch' and are longer than 4 characters\n",
    "for word in word_list:\n",
    "    if word.startswith('ch'):\n",
    "        start_with_ch_words.append(word)\n",
    "    if len(word) > 4:\n",
    "        long_words.append(word)\n",
    "\n",
    "# Step 3: Print results\n",
    "print(start_with_ch_words)\n",
    "print(long_words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chair', 'chapter', 'china', 'character']\n",
      "['chair', 'teacher', 'chapter', 'cider', 'china', 'camera', 'character', 'beach', 'approach']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOUi8BqB08CE"
   },
   "source": [
    "### Problem 5 (3 points)\n",
    "\n",
    "We want to identify special bigrams that occur in the same sentence, with exactly two words between them. For example, in the sentence \"Success comes to those who work.\", the following three special bigram pairs are considered special:\n",
    "- \\(```Success```, ```those```\\)\n",
    "- \\(```comes```, ```who```\\)\n",
    "- \\(```to```, ```work```\\)\n",
    "\n",
    "Find the 10 most frequent special bigrams in the News category of the Brown Corpus and show the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-Nqx3_tx07jE",
    "ExecuteTime": {
     "end_time": "2025-03-25T12:35:58.624563Z",
     "start_time": "2025-03-25T12:35:58.512714Z"
    }
   },
   "source": [
    "# Problem 5\n",
    "\n",
    "# Step 1: Load Brown Corpus's News Category\n",
    "from nltk.corpus import brown\n",
    "news_sents = brown.sents(categories='news')\n",
    "\n",
    "# Step 2: Declare a dictionary to store the bigram frequency\n",
    "bigram_freq = {}\n",
    "\n",
    "# Step 3: Find special bigrams with only alphabet characters\n",
    "for sent in news_sents:\n",
    "    for i in range(len(sent) - 3):\n",
    "        if sent[i].isalpha() and sent[i+1].isalpha() and sent[i+2].isalpha() and sent[i+3].isalpha():\n",
    "            bigram = (sent[i], sent[i+3])\n",
    "            bigram_freq[bigram] = bigram_freq.get(bigram, 0) + 1\n",
    "            \n",
    "# Step 4: Sort bigrams by frequency\n",
    "sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 5: Print top 10 bigrams\n",
    "print(sorted_bigrams[:10])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'the'), 345), (('the', 'of'), 228), (('a', 'the'), 127), (('the', 'a'), 104), (('the', 'in'), 99), (('and', 'of'), 97), (('to', 'the'), 97), (('in', 'of'), 95), (('and', 'the'), 91), (('to', 'of'), 79)]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2MHKTnN1Eld"
   },
   "source": [
    "### Problem 6 (3 points)\n",
    "\n",
    "Show how many words in the Gutenberg shakespeare-macbeth corpus have two or more pronunciations according to the CMU Pronouncing Dictionary. Additionally, show the list of words with the most diverse pronunciations.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fkj9fJYW1DzD",
    "ExecuteTime": {
     "end_time": "2025-03-25T23:33:22.462811Z",
     "start_time": "2025-03-25T23:33:22.118104Z"
    }
   },
   "source": [
    "# Problem 6\n",
    "\n",
    "# Step 1: Load Shakespeare Macbeth Corpus and preprocess the words\n",
    "from nltk.corpus import gutenberg\n",
    "macbeth_words = set(\n",
    "    w.lower()\n",
    "    for w in gutenberg.words('shakespeare-macbeth.txt')\n",
    "    if w.isalpha()\n",
    ")\n",
    "\n",
    "# Step 2: Load CMU Pronouncing Dictionary\n",
    "from nltk.corpus import cmudict\n",
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "# Step 3: Find words with >= 2 pronunciations\n",
    "multi_pron_words = []\n",
    "for word in macbeth_words:\n",
    "    if word in cmu_dict and len(cmu_dict[word]) >= 2:\n",
    "        multi_pron_words.append(word)\n",
    "        \n",
    "# Step 4: Print the total number of words with >= 2 pronunciations\n",
    "print(len(multi_pron_words))\n",
    "\n",
    "# Step 5: Find word with the most diverse pronunciations\n",
    "max_pronunciation_word = []\n",
    "max_pronunciation_count = 0\n",
    "for word in multi_pron_words:\n",
    "    if len(cmu_dict[word]) > max_pronunciation_count:\n",
    "        max_pronunciation_count = len(cmu_dict[word])\n",
    "        max_pronunciation_word = [word]\n",
    "    elif len(cmu_dict[word]) == max_pronunciation_count:\n",
    "        max_pronunciation_word.append(word)\n",
    "\n",
    "# Step 6: Print the list of words with the most diverse pronunciations\n",
    "print(max_pronunciation_word)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "['effects', 'painted', 'direction', 'interest', 'planted', 'ne', 'when', 'with', 'was', 'directly']\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecuw59je1I6D"
   },
   "source": [
    "### Problem 7 (4 points)\n",
    "\n",
    "\n",
    "Provide a brief explanation for each of the following questions.\n",
    "\n",
    "a. Retrieve all noun synsets for ```duck``` from WordNet. For each synset, print all hypernyms and the lemma names of those hypernyms. (2 points)\n",
    "\n",
    "b. Using ```wn.all_synsets('v')```, write a Python code that prints 5 random lemmas from the first 10 verb synsets. (The output lemma will be in the format: ```Lemma('run.v.02.run')```) (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1wt6adZ1YWk"
   },
   "outputs": [],
   "source": [
    "# Problem 7\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FvQLJEM1aI8"
   },
   "source": [
    "### Problem 8 (3 points)\n",
    "\n",
    "Describe the class of strings that match each of the following regular expressions.\n",
    "1.\t[a-zA]+\n",
    "2.\t[a-z][A-Z]*[a-z]\n",
    "3.\t[BCLT]r[aeiou]{,2}t[a-z]\n",
    "4.\t([aeiou][^aeiou])*\n",
    "5.\t\\W+|[^\\W\\s]+\n",
    "\n",
    "You can test your answers using ```nltk.re_show()```.\n",
    "Your answers should be included in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrkrUCOV1iQ8"
   },
   "outputs": [],
   "source": [
    "# Problem 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNet6GaG10Gk"
   },
   "source": [
    "### Problem 9 (5 points)\n",
    "\n",
    "Pig Latin is a simple transformation of English text. Each word is converted as follows:\n",
    "- Move all consonant (or consonant cluster) at the start of a word to the end.\n",
    "- Append ```ay``` to the word.\n",
    "- Example:\n",
    "  - ```string``` → ```ingstray```\n",
    "  - ```language``` → ```anguagelay```\n",
    "  - ```idle``` → ```idleay```\n",
    "  - For more details, refer to http://en.wikipedia.org/wiki/Pig_Latin.\n",
    "\n",
    "This time, we will modify the Pig Latin rules:\n",
    "- After moving the consonant(s) of the word, change the first vowel in the word:\n",
    "    - ```a``` → ```e```\n",
    "    - ```e``` → ```i```\n",
    "    - ```i``` → ```o```\n",
    "    - ```o``` → ```u```\n",
    "    - ```u``` → ```a```\n",
    "- Append ```ay``` to the word.\n",
    "- Example:\n",
    "    - ```string``` → ```ongstray```\n",
    "    - ```idle``` → ```odleay```\n",
    "\n",
    "a. Write a Python function to convert a word into its equivalent in the modified Pig Latin. (3 points)\n",
    "\n",
    "b. Write a Python code that converts an entire text (a sequence of words), instead of just individual words.  (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irIbiO6U19Mc"
   },
   "outputs": [],
   "source": [
    "# Problem 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9tST3r52Dhk"
   },
   "source": [
    "### Problem 10 (4 points)\n",
    "\n",
    "Using a multilingual corpus such as the Universal Declaration of Human Rights Corpus (```nltk.corpus.udhr```), along with NLTK’s frequency distribution (```nltk.FreqDist```), develop a system that guesses the language of a previously unseen text.\n",
    "\n",
    "You can estimate the language of the unseen text by comparing:\n",
    "- How often each word appears in each language of the corpus\n",
    "- How often each word appears in the unseen text.\n",
    "\n",
    "To achieve this, use rank correlation (```nltk.spearman_correlation```).\n",
    "\n",
    "For simplicity, you can work at the character level rather than the word level and choose languages (at least two) of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyeLRM_2BKk"
   },
   "outputs": [],
   "source": [
    "# Problem 10\n",
    "from nltk.metrics.spearman import spearman_correlation\n",
    "\n",
    "unseen_text = \"You never really understand a person until you consider things from his point of view… Until you climb inside of his skin and walk around in it.\" # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNwK-R-Z2Nnt"
   },
   "source": [
    "### Problem 11 (6 points)\n",
    "\n",
    "Write a Python program that processes a text and discovers cases where a word is used with a novel sense.\n",
    "\n",
    "One simple approach is using WordNet:\n",
    "For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context.\n",
    "\n",
    "Note: this is a crude approach. Also, it is important to mention that implementing this effectively is a difficult, open research problem.\n",
    "\n",
    "Define what a “novel sense” of the word is in your own words, and describe the detailed design choice you made for your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY2Xrs2j2UON"
   },
   "outputs": [],
   "source": [
    "# Problem 11\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
